{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyO9Y/qsbO4xHIU0ty83OhF2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Random Forest"],"metadata":{"id":"b-wcEL6_2kKa"}},{"cell_type":"code","source":["import numpy as np\n","from collections import Counter\n","\n","\n","\n","def gini_impurity(y):\n","    \"\"\"\n","    Calculate the Gini Impurity for an array of class labels.\n","    \"\"\"\n","    if len(y) == 0:\n","        return 0\n","    counts = np.bincount(y)\n","    probabilities = counts / len(y)\n","    return 1 - np.sum(probabilities ** 2)\n","\n","def best_split(X, y, feature_indices):\n","    \"\"\"\n","    Find the best split for the data in X, y using only the features in feature_indices.\n","    Returns:\n","      - best_feature: index of the best feature to split on\n","      - best_threshold: threshold value for the split\n","      - best_gain: the impurity gain from the best split\n","      - best_left_idx, best_right_idx: indices of the data points going to left and right splits\n","    \"\"\"\n","    best_feature = None\n","    best_threshold = None\n","    best_gain = -np.inf\n","    best_left_idx = None\n","    best_right_idx = None\n","    parent_impurity = gini_impurity(y)\n","    n_samples = X.shape[0]\n","\n","    if n_samples < 2:\n","        return None, None, None, None, None\n","\n","    for feature in feature_indices:\n","        # Get all unique values for this feature as potential thresholds\n","        thresholds = np.unique(X[:, feature])\n","        for threshold in thresholds:\n","            # Split the dataset into left/right groups\n","            left_idx = np.where(X[:, feature] <= threshold)[0]\n","            right_idx = np.where(X[:, feature] > threshold)[0]\n","            if len(left_idx) == 0 or len(right_idx) == 0:\n","                continue  # skip useless splits\n","\n","            # Calculate impurity for children nodes\n","            left_impurity = gini_impurity(y[left_idx])\n","            right_impurity = gini_impurity(y[right_idx])\n","            # Compute weighted impurity of the split\n","            weighted_impurity = (len(left_idx) / n_samples) * left_impurity + \\\n","                                (len(right_idx) / n_samples) * right_impurity\n","            gain = parent_impurity - weighted_impurity\n","\n","            if gain > best_gain:\n","                best_gain = gain\n","                best_feature = feature\n","                best_threshold = threshold\n","                best_left_idx = left_idx\n","                best_right_idx = right_idx\n","\n","    return best_feature, best_threshold, best_gain, best_left_idx, best_right_idx\n","\n","class Node:\n","    \"\"\"\n","    A node in the decision tree.\n","    \"\"\"\n","    def __init__(self, feature=None, threshold=None, left=None, right=None, *, value=None):\n","        # For internal nodes:\n","        self.feature = feature      # index of feature to split on\n","        self.threshold = threshold  # threshold value for the split\n","        self.left = left            # left child node (X[feature] <= threshold)\n","        self.right = right          # right child node (X[feature] > threshold)\n","        # For leaf nodes:\n","        self.value = value          # class label (for classification)\n","\n","# -------------------------------\n","# Decision Tree Implementation\n","# -------------------------------\n","\n","def build_tree(X, y, max_depth=None, min_samples_split=2, n_features=None, depth=0):\n","    \"\"\"\n","    Recursively build a decision tree.\n","    - X: features (numpy array of shape [n_samples, n_features])\n","    - y: labels (numpy array of shape [n_samples])\n","    - max_depth: maximum depth of the tree (None means unlimited)\n","    - min_samples_split: minimum number of samples required to split\n","    - n_features: number of random features to consider at each split (if None, use all)\n","    - depth: current depth of the tree\n","    \"\"\"\n","    n_samples, n_total_features = X.shape\n","    num_labels = len(np.unique(y))\n","\n","    # Stopping conditions: max depth reached, pure node, or too few samples to split.\n","    if (max_depth is not None and depth >= max_depth) or num_labels == 1 or n_samples < min_samples_split:\n","        # Create a leaf node with the most common label\n","        leaf_value = Counter(y).most_common(1)[0][0]\n","        return Node(value=leaf_value)\n","\n","    # Randomly select subset of features if specified\n","    if n_features is None:\n","        feature_indices = list(range(n_total_features))\n","    else:\n","        feature_indices = np.random.choice(n_total_features, n_features, replace=False)\n","\n","    # Find the best split among the chosen features\n","    best = best_split(X, y, feature_indices)\n","    feature, threshold, gain, left_idx, right_idx = best\n","\n","    # If no valid split was found, create a leaf node\n","    if feature is None or gain == 0:\n","        leaf_value = Counter(y).most_common(1)[0][0]\n","        return Node(value=leaf_value)\n","\n","    # Recursively build the left and right subtrees\n","    left_subtree = build_tree(X[left_idx, :], y[left_idx],\n","                              max_depth=max_depth,\n","                              min_samples_split=min_samples_split,\n","                              n_features=n_features,\n","                              depth=depth+1)\n","    right_subtree = build_tree(X[right_idx, :], y[right_idx],\n","                               max_depth=max_depth,\n","                               min_samples_split=min_samples_split,\n","                               n_features=n_features,\n","                               depth=depth+1)\n","\n","    return Node(feature=feature, threshold=threshold, left=left_subtree, right=right_subtree)\n","\n","def predict_tree(x, tree):\n","    \"\"\"\n","    Predict the class label for a single sample x using the decision tree.\n","    \"\"\"\n","    # If the node is a leaf, return its value\n","    if tree.value is not None:\n","        return tree.value\n","    # Otherwise, follow the appropriate branch\n","    if x[tree.feature] <= tree.threshold:\n","        return predict_tree(x, tree.left)\n","    else:\n","        return predict_tree(x, tree.right)\n","\n","# -------------------------------\n","# Random Forest Implementation\n","# -------------------------------\n","\n","class RandomForest:\n","    def __init__(self, n_trees=10, max_depth=None, min_samples_split=2, n_features=None, bootstrap=True):\n","        \"\"\"\n","        Parameters:\n","          - n_trees: Number of trees in the forest.\n","          - max_depth: Maximum depth for each tree.\n","          - min_samples_split: Minimum samples required to split a node.\n","          - n_features: Number of features to consider when looking for the best split.\n","                        (If None, then use all features.)\n","          - bootstrap: Whether to use bootstrap samples when building trees.\n","        \"\"\"\n","        self.n_trees = n_trees\n","        self.max_depth = max_depth\n","        self.min_samples_split = min_samples_split\n","        self.n_features = n_features\n","        self.bootstrap = bootstrap\n","        self.trees = []\n","\n","    def fit(self, X, y):\n","        \"\"\"\n","        Build the forest of trees from the training set.\n","        \"\"\"\n","        self.trees = []\n","        n_samples = X.shape[0]\n","        for _ in range(self.n_trees):\n","            # Create a bootstrap sample\n","            if self.bootstrap:\n","                indices = np.random.choice(n_samples, n_samples, replace=True)\n","                X_sample = X[indices]\n","                y_sample = y[indices]\n","            else:\n","                X_sample = X\n","                y_sample = y\n","            # Build a decision tree on the sample\n","            tree = build_tree(X_sample, y_sample,\n","                              max_depth=self.max_depth,\n","                              min_samples_split=self.min_samples_split,\n","                              n_features=self.n_features)\n","            self.trees.append(tree)\n","\n","    def predict(self, X):\n","        \"\"\"\n","        Predict class labels for samples in X.\n","        Returns an array of predictions using majority voting.\n","        \"\"\"\n","        # Collect predictions from each tree\n","        tree_preds = np.array([[predict_tree(x, tree) for tree in self.trees] for x in X])\n","        # Majority vote: for each sample, choose the most common class prediction\n","        y_pred = [Counter(tree_pred).most_common(1)[0][0] for tree_pred in tree_preds]\n","        return np.array(y_pred)\n"],"metadata":{"id":"wwpCKYte2mZZ","executionInfo":{"status":"ok","timestamp":1758897956039,"user_tz":240,"elapsed":18,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}}},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["# Gradient Boosting\n","\n","##  Problem Setup\n","\n","Given a dataset $\\{(x_i, y_i)\\}_{i=1}^N$ with $x_i \\in \\mathbb{R}^d$ and $y_i \\in \\mathbb{R}$ (for regression), we aim to minimize:\n","\n","$$\n","\\mathbf{L}(F) = \\sum_{i=1}^N L(y_i, F(x_i))\n","$$\n","\n","where $F(x)$ is an additive model:\n","\n","$$\n","F(x) = \\sum_{m=0}^{M} \\gamma_m h_m(x).\n","$$\n","\n","## Functional Gradient Descent\n","\n","Gradient descent in parameter space:\n","\n","$$\n","\\theta_m = \\theta_{m-1} - \\eta \\nabla_\\theta \\mathbf{L}(\\theta_{m-1}).\n","$$\n","\n","Gradient descent in function space:\n","\n","$$\n","F_m(x) = F_{m-1}(x) - \\eta \\nabla_F \\mathbf{L}(F_{m-1}).\n","$$\n","\n","At each step, we approximate the negative gradient:\n","\n","$$\n","r_{im} = - \\left. \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)} \\right|_{F = F_{m-1}}.\n","$$\n","\n","and fit a weak learner:\n","\n","$$\n","h_m = \\arg \\min_{h \\in \\mathbf{H}} \\sum_{i=1}^N (r_{im} - h(x_i))^2.\n","$$\n","\n","##  Algorithm\n","\n","1. **Initialize:**  \n","   $F_0(x) = \\arg \\min_\\gamma \\sum_{i=1}^N L(y_i, \\gamma)$.\n","2. **For** $m = 1, \\dots, M$:\n","   - Compute pseudo-residuals:\n","     $$\n","     r_{im} = - \\frac{\\partial L(y_i, F(x_i))}{\\partial F(x_i)}.\n","     $$\n","   - Fit weak learner $h_m$ to $\\{(x_i, r_{im})\\}$.\n","   - Find optimal weight:\n","     $$\n","     \\gamma_m = \\arg \\min_{\\gamma} \\sum_{i=1}^N L(y_i, F_{m-1}(x_i) + \\gamma h_m(x_i)).\n","     $$\n","   - Update:\n","     $$\n","     F_m(x) = F_{m-1}(x) + \\nu \\gamma_m h_m(x).\n","     $$\n","\n","## Example: Squared Error Loss\n","\n","For $L(y, F) = \\frac{1}{2} (y - F)^2$, we get:\n","\n","$$\n","r_{im} = y_i - F_{m-1}(x_i).\n","$$\n","\n","Thus, gradient boosting reduces to stage-wise residual fitting."],"metadata":{"id":"cRSbGJ0Cni97"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"JcSHL-XUnc-5","colab":{"base_uri":"https://localhost:8080/","height":487},"executionInfo":{"status":"ok","timestamp":1758897978400,"user_tz":240,"elapsed":4437,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"c1c4b01f-321d-46ab-d8bc-e1090c4f7f19"},"outputs":[{"output_type":"stream","name":"stdout","text":["Regression MSE: 220.5909\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Classification Accuracy: 0.9600\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n","from sklearn.datasets import make_regression, make_classification\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error, accuracy_score\n","\n","# ---- Regression Example ----\n","X_reg, y_reg = make_regression(n_samples=500, n_features=1, noise=15, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X_reg, y_reg, test_size=0.2, random_state=42)\n","\n","gbr = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n","gbr.fit(X_train, y_train)\n","\n","y_pred = gbr.predict(X_test)\n","print(f\"Regression MSE: {mean_squared_error(y_test, y_pred):.4f}\")\n","\n","# Plot regression results\n","plt.scatter(X_test, y_test, color=\"blue\", label=\"True\")\n","plt.scatter(X_test, y_pred, color=\"red\", alpha=0.6, label=\"Predicted\")\n","plt.title(\"Gradient Boosting Regression\")\n","plt.legend()\n","plt.show()\n","\n","# ---- Classification Example ----\n","X_cls, y_cls = make_classification(n_samples=500, n_features=5, random_state=42)\n","X_train, X_test, y_train, y_test = train_test_split(X_cls, y_cls, test_size=0.2, random_state=42)\n","\n","gbc = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=42)\n","gbc.fit(X_train, y_train)\n","\n","y_pred = gbc.predict(X_test)\n","print(f\"Classification Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n"]},{"cell_type":"markdown","source":["# XGBoost\n","\n","\n","## Objective Function\n","\n","Given data $\\{(x_i, y_i)\\}_{i=1}^n$ and loss $\\ell(y_i, \\hat{y}_i)$, XGBoost minimizes:\n","$$\n","\\mathcal{L}(F) = \\sum_{i=1}^n \\ell(y_i, \\hat{y}_i) + \\sum_{t=1}^T \\Omega(f_t),\n","$$\n","where $\\Omega(f_t)$ penalizes tree complexity:\n","$$\n","\\Omega(f_t) = \\gamma T_{\\text{leaf}} + \\frac{1}{2} \\lambda \\sum_{j=1}^{T_{\\text{leaf}}} w_j^2.\n","$$\n","\n","## Second-Order Approximation\n","\n","For a new tree $f_t$, predictions update as:\n","$$\n","\\hat{y}_i^{(t)} = \\hat{y}_i^{(t-1)} + f_t(x_i).\n","$$\n","Using Taylor expansion:\n","$$\n","\\ell(y_i, \\hat{y}_i^{(t)}) \\approx \\ell(y_i, \\hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2,\n","$$\n","where\n","$$\n","g_i = \\frac{\\partial \\ell}{\\partial \\hat{y}_i}, \\quad h_i = \\frac{\\partial^2 \\ell}{\\partial \\hat{y}_i^2}.\n","$$\n","Thus, the objective reduces to:\n","$$\n","\\mathcal{L}^{(t)} \\approx \\sum_{i=1}^n (g_i f_t(x_i) + \\frac{1}{2} h_i f_t(x_i)^2) + \\Omega(f_t).\n","$$\n","\n","##  Tree Structure and Optimal Weights\n","\n","Each tree assigns samples to leaves:\n","$$\n","f_t(x) = w_{q(x)},\n","$$\n","where $q(x)$ maps $x$ to a leaf index. Let:\n","$$\n","G_j = \\sum_{i \\in I_j} g_i, \\quad H_j = \\sum_{i \\in I_j} h_i.\n","$$\n","Then the objective simplifies to:\n","$$\n","\\mathcal{L}^{(t)} \\approx \\sum_{j=1}^{T_{\\text{leaf}}} \\left(G_j w_j + \\frac{1}{2} H_j w_j^2 \\right) + \\gamma T_{\\text{leaf}} + \\frac{1}{2}\\lambda \\sum_{j=1}^{T_{\\text{leaf}}} w_j^2.\n","$$\n","\n","###  Computing Optimal Leaf Weights\n","\n","To find the best leaf weights $\\{ w_j \\}$, we minimize the objective with respect to each $w_j$:\n","$$\n","\\frac{\\partial}{\\partial w_j} \\left(G_j w_j + \\frac{1}{2} H_j w_j^2 + \\frac{1}{2} \\lambda w_j^2 \\right) = G_j + (H_j + \\lambda) w_j = 0.\n","$$\n","Solving for $w_j^*$:\n","$$\n","w_j^* = -\\frac{G_j}{H_j + \\lambda}.\n","$$\n","\n","###  Gain from Adding a Tree\n","\n","Plugging $w_j^*$ into the objective, the reduction in loss (gain) from adding a tree is:\n","$$\n","\\text{Gain} = -\\frac{1}{2} \\sum_{j=1}^{T_{\\text{leaf}}} \\frac{G_j^2}{H_j + \\lambda} + \\gamma T_{\\text{leaf}}.\n","$$\n","A larger gain means a better split.\n","\n","\n","## Split Finding\n","\n","For a potential split into left ($I_L$) and right ($I_R$):\n","$$\n","\\text{Gain} = \\frac{1}{2} \\left( \\frac{G_L^2}{H_L + \\lambda} + \\frac{G_R^2}{H_R + \\lambda} - \\frac{G^2}{H + \\lambda} \\right) - \\gamma.\n","$$\n","XGBoost picks the split maximizing this gain.\n","\n","\n","##  Mean Squared Error (MSE) Loss\n","\n","For squared loss:\n","$$\n","\\ell(y_i, \\hat{y}_i) = \\frac{1}{2} (y_i - \\hat{y}_i)^2.\n","$$\n","Gradients:\n","$$\n","g_i = \\frac{\\partial}{\\partial \\hat{y}_i} \\left[\\frac{1}{2} (y_i - \\hat{y}_i)^2\\right] = \\hat{y}_i - y_i,\n","$$\n","$$\n","h_i = \\frac{\\partial^2}{\\partial \\hat{y}_i^2} \\left[\\frac{1}{2} (y_i - \\hat{y}_i)^2\\right] = 1.\n","$$\n","Then:\n","$$\n","G_j = \\sum_{i \\in I_j} (\\hat{y}_i - y_i), \\quad H_j = |I_j|.\n","$$\n","Optimal leaf weight:\n","$$\n","w_j^* = -\\frac{\\sum_{i \\in I_j} (\\hat{y}_i - y_i)}{|I_j| + \\lambda}.\n","$$\n","\n"],"metadata":{"id":"LkEAVNVFpZbm"}},{"cell_type":"code","source":["import xgboost as xgb\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import numpy as np\n","\n","# ------------------------------\n","# Load and preprocess MNIST data\n","# ------------------------------\n","\n","# Fetch the MNIST dataset from OpenML\n","mnist = fetch_openml('mnist_784', version=1, as_frame=False)\n","X, y = mnist.data, mnist.target\n","\n","# Convert target labels to integers (they may come as strings)\n","y = y.astype(np.int32)\n","\n","# Normalize pixel values to the range [0, 1]\n","X = X / 255.0\n","\n","# Split the data into training and testing sets.\n","# Here we use 60,000 samples for training and 10,000 for testing.\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=10000, random_state=42, stratify=y\n",")\n","\n","# ------------------------------\n","# Train using XGBClassifier\n","# ------------------------------\n","\n","# Initialize the XGBClassifier.\n","# - 'objective' is set to 'multi:softprob' to get class probabilities.\n","# - 'num_class' is set to 10 (digits 0 through 9).\n","# - 'use_label_encoder' is disabled to avoid warnings (XGBoost 1.3+).\n","# - 'eval_metric' is set to 'mlogloss' (multiclass log-loss).\n","clf = xgb.XGBClassifier(\n","    objective='multi:softprob',\n","    num_class=10,\n","    learning_rate=0.1,\n","    max_depth=6,\n","    n_estimators=100,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    random_state=42,\n","    use_label_encoder=False,  # Avoids deprecation warnings in newer versions.\n","    eval_metric='mlogloss'\n",")\n","\n","# Set up an evaluation set for early stopping.\n","eval_set = [(X_test, y_test)]\n","\n","# Train the model with early stopping if there is no improvement after 10 rounds.\n","clf.fit(X_train, y_train,  eval_set=eval_set, verbose=True)\n","\n","# Make predictions on the test set.\n","y_pred = clf.predict(X_test)\n","\n","# Evaluate and print the accuracy.\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"XGBClassifier Accuracy on MNIST: {:.2f}%\".format(accuracy * 100))\n"],"metadata":{"id":"9faxnH4Up3Z1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758898626244,"user_tz":240,"elapsed":566766,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"e4e40f6d-000b-46d6-e382-9b3f735b3aea"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [14:47:59] WARNING: /workspace/src/learner.cc:738: \n","Parameters: { \"use_label_encoder\" } are not used.\n","\n","  bst.update(dtrain, iteration=i, fobj=obj)\n"]},{"output_type":"stream","name":"stdout","text":["[0]\tvalidation_0-mlogloss:1.96966\n","[1]\tvalidation_0-mlogloss:1.73759\n","[2]\tvalidation_0-mlogloss:1.55649\n","[3]\tvalidation_0-mlogloss:1.41114\n","[4]\tvalidation_0-mlogloss:1.28926\n","[5]\tvalidation_0-mlogloss:1.18460\n","[6]\tvalidation_0-mlogloss:1.09344\n","[7]\tvalidation_0-mlogloss:1.01279\n","[8]\tvalidation_0-mlogloss:0.94136\n","[9]\tvalidation_0-mlogloss:0.87810\n","[10]\tvalidation_0-mlogloss:0.82030\n","[11]\tvalidation_0-mlogloss:0.76680\n","[12]\tvalidation_0-mlogloss:0.72017\n","[13]\tvalidation_0-mlogloss:0.67736\n","[14]\tvalidation_0-mlogloss:0.63858\n","[15]\tvalidation_0-mlogloss:0.60291\n","[16]\tvalidation_0-mlogloss:0.57040\n","[17]\tvalidation_0-mlogloss:0.54046\n","[18]\tvalidation_0-mlogloss:0.51265\n","[19]\tvalidation_0-mlogloss:0.48705\n","[20]\tvalidation_0-mlogloss:0.46426\n","[21]\tvalidation_0-mlogloss:0.44253\n","[22]\tvalidation_0-mlogloss:0.42272\n","[23]\tvalidation_0-mlogloss:0.40413\n","[24]\tvalidation_0-mlogloss:0.38703\n","[25]\tvalidation_0-mlogloss:0.37096\n","[26]\tvalidation_0-mlogloss:0.35607\n","[27]\tvalidation_0-mlogloss:0.34261\n","[28]\tvalidation_0-mlogloss:0.33019\n","[29]\tvalidation_0-mlogloss:0.31796\n","[30]\tvalidation_0-mlogloss:0.30676\n","[31]\tvalidation_0-mlogloss:0.29631\n","[32]\tvalidation_0-mlogloss:0.28671\n","[33]\tvalidation_0-mlogloss:0.27787\n","[34]\tvalidation_0-mlogloss:0.26914\n","[35]\tvalidation_0-mlogloss:0.26066\n","[36]\tvalidation_0-mlogloss:0.25287\n","[37]\tvalidation_0-mlogloss:0.24545\n","[38]\tvalidation_0-mlogloss:0.23858\n","[39]\tvalidation_0-mlogloss:0.23198\n","[40]\tvalidation_0-mlogloss:0.22608\n","[41]\tvalidation_0-mlogloss:0.22036\n","[42]\tvalidation_0-mlogloss:0.21517\n","[43]\tvalidation_0-mlogloss:0.21041\n","[44]\tvalidation_0-mlogloss:0.20589\n","[45]\tvalidation_0-mlogloss:0.20150\n","[46]\tvalidation_0-mlogloss:0.19730\n","[47]\tvalidation_0-mlogloss:0.19328\n","[48]\tvalidation_0-mlogloss:0.18934\n","[49]\tvalidation_0-mlogloss:0.18568\n","[50]\tvalidation_0-mlogloss:0.18209\n","[51]\tvalidation_0-mlogloss:0.17870\n","[52]\tvalidation_0-mlogloss:0.17555\n","[53]\tvalidation_0-mlogloss:0.17251\n","[54]\tvalidation_0-mlogloss:0.16932\n","[55]\tvalidation_0-mlogloss:0.16663\n","[56]\tvalidation_0-mlogloss:0.16352\n","[57]\tvalidation_0-mlogloss:0.16104\n","[58]\tvalidation_0-mlogloss:0.15863\n","[59]\tvalidation_0-mlogloss:0.15618\n","[60]\tvalidation_0-mlogloss:0.15394\n","[61]\tvalidation_0-mlogloss:0.15174\n","[62]\tvalidation_0-mlogloss:0.14962\n","[63]\tvalidation_0-mlogloss:0.14773\n","[64]\tvalidation_0-mlogloss:0.14578\n","[65]\tvalidation_0-mlogloss:0.14386\n","[66]\tvalidation_0-mlogloss:0.14196\n","[67]\tvalidation_0-mlogloss:0.14020\n","[68]\tvalidation_0-mlogloss:0.13837\n","[69]\tvalidation_0-mlogloss:0.13652\n","[70]\tvalidation_0-mlogloss:0.13503\n","[71]\tvalidation_0-mlogloss:0.13321\n","[72]\tvalidation_0-mlogloss:0.13169\n","[73]\tvalidation_0-mlogloss:0.13010\n","[74]\tvalidation_0-mlogloss:0.12880\n","[75]\tvalidation_0-mlogloss:0.12738\n","[76]\tvalidation_0-mlogloss:0.12617\n","[77]\tvalidation_0-mlogloss:0.12482\n","[78]\tvalidation_0-mlogloss:0.12349\n","[79]\tvalidation_0-mlogloss:0.12239\n","[80]\tvalidation_0-mlogloss:0.12140\n","[81]\tvalidation_0-mlogloss:0.12017\n","[82]\tvalidation_0-mlogloss:0.11898\n","[83]\tvalidation_0-mlogloss:0.11770\n","[84]\tvalidation_0-mlogloss:0.11672\n","[85]\tvalidation_0-mlogloss:0.11558\n","[86]\tvalidation_0-mlogloss:0.11436\n","[87]\tvalidation_0-mlogloss:0.11348\n","[88]\tvalidation_0-mlogloss:0.11253\n","[89]\tvalidation_0-mlogloss:0.11147\n","[90]\tvalidation_0-mlogloss:0.11057\n","[91]\tvalidation_0-mlogloss:0.10975\n","[92]\tvalidation_0-mlogloss:0.10893\n","[93]\tvalidation_0-mlogloss:0.10798\n","[94]\tvalidation_0-mlogloss:0.10729\n","[95]\tvalidation_0-mlogloss:0.10658\n","[96]\tvalidation_0-mlogloss:0.10584\n","[97]\tvalidation_0-mlogloss:0.10513\n","[98]\tvalidation_0-mlogloss:0.10434\n","[99]\tvalidation_0-mlogloss:0.10363\n","XGBClassifier Accuracy on MNIST: 97.03%\n"]}]},{"cell_type":"code","source":["# Get the booster from the trained model\n","booster = clf.get_booster()\n","\n","# Get feature importance scores by 'gain'\n","importance_gain = booster.get_score(importance_type='gain')\n","print(\"\\nFeature Importances (Booster by Gain):\")\n","for feature, score in importance_gain.items():\n","    print(f\"{feature}: {score:.4f}\")\n","\n","# Optionally, get feature importance by 'weight'\n","importance_weight = booster.get_score(importance_type='weight')\n","print(\"\\nFeature Importances (Booster by Weight):\")\n","for feature, score in importance_weight.items():\n","    print(f\"{feature}: {score}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T035Hr6T9WLo","executionInfo":{"status":"ok","timestamp":1758898639157,"user_tz":240,"elapsed":15,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"72fcbaba-f710-400b-da7c-a3820394e736"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Feature Importances (Booster by Gain):\n","f43: 0.7801\n","f44: 0.8413\n","f45: 0.7382\n","f66: 209.5063\n","f67: 1.6797\n","f68: 3.6039\n","f69: 127.8272\n","f70: 15.3886\n","f71: 1.8951\n","f72: 0.7592\n","f73: 0.9414\n","f74: 6.9372\n","f75: 5.1857\n","f76: 8.0437\n","f91: 6.5453\n","f92: 24.6509\n","f93: 38.9295\n","f94: 97.2169\n","f95: 39.5877\n","f96: 102.5253\n","f97: 60.2205\n","f98: 43.0340\n","f99: 38.8932\n","f100: 196.0658\n","f101: 263.4692\n","f102: 3.2953\n","f103: 87.7536\n","f104: 67.5673\n","f105: 4.0847\n","f106: 32.6529\n","f107: 16.2954\n","f108: 22.9668\n","f117: 2.7063\n","f118: 0.6703\n","f119: 2.0409\n","f120: 3.2424\n","f121: 3.7695\n","f122: 5.1171\n","f123: 7.2589\n","f124: 44.1661\n","f125: 30.7760\n","f126: 44.9572\n","f127: 11.6935\n","f128: 10.1957\n","f129: 4.2349\n","f130: 4.4879\n","f131: 5.2684\n","f132: 4.9363\n","f133: 3.6200\n","f134: 5.3379\n","f135: 7.3938\n","f136: 1.1070\n","f137: 1.0595\n","f143: 0.8043\n","f144: 1.8642\n","f145: 1.1745\n","f146: 2.7340\n","f147: 4.4092\n","f148: 5.2990\n","f149: 18.7248\n","f150: 14.8288\n","f151: 7.3491\n","f152: 10.0654\n","f153: 18.9435\n","f154: 24.6561\n","f155: 139.4860\n","f156: 105.4243\n","f157: 12.6328\n","f158: 18.8394\n","f159: 8.1405\n","f160: 5.9956\n","f161: 3.2209\n","f162: 4.0890\n","f163: 33.5603\n","f164: 11.6283\n","f165: 2.4947\n","f166: 2.6000\n","f171: 0.8289\n","f172: 1.4380\n","f173: 3.6686\n","f174: 12.1159\n","f175: 7.6239\n","f176: 13.0155\n","f177: 69.4291\n","f178: 32.1027\n","f179: 18.8708\n","f180: 9.5249\n","f181: 7.5044\n","f182: 11.5081\n","f183: 14.8357\n","f184: 9.3104\n","f185: 10.8124\n","f186: 13.1367\n","f187: 11.6636\n","f188: 9.7392\n","f189: 4.0424\n","f190: 9.7302\n","f191: 13.0339\n","f192: 3.9684\n","f193: 1.3574\n","f199: 1.7264\n","f200: 2.5758\n","f201: 5.0452\n","f202: 3.5004\n","f203: 16.9259\n","f204: 8.5769\n","f205: 24.9032\n","f206: 16.0151\n","f207: 8.4895\n","f208: 13.9479\n","f209: 16.3953\n","f210: 43.0528\n","f211: 171.8026\n","f212: 21.1104\n","f213: 16.4624\n","f214: 10.2039\n","f215: 8.4581\n","f216: 13.1945\n","f217: 40.1765\n","f218: 16.5786\n","f219: 42.3443\n","f220: 206.9300\n","f221: 1.1525\n","f222: 1.5223\n","f227: 1.0875\n","f228: 3.5503\n","f229: 2.1549\n","f230: 9.5764\n","f231: 44.7581\n","f232: 7.0591\n","f233: 13.8539\n","f234: 36.9689\n","f235: 49.2917\n","f236: 7.0081\n","f237: 7.4728\n","f238: 6.9131\n","f239: 9.6897\n","f240: 5.6844\n","f241: 7.5824\n","f242: 60.3298\n","f243: 45.8200\n","f244: 12.7743\n","f245: 29.5625\n","f246: 7.0108\n","f247: 5.0845\n","f248: 5.0572\n","f249: 2.4821\n","f250: 3.5395\n","f255: 1.0484\n","f256: 3.8248\n","f257: 2.2010\n","f258: 2.3474\n","f259: 3.2809\n","f260: 6.0545\n","f261: 9.0260\n","f262: 28.4898\n","f263: 6.2842\n","f264: 7.3527\n","f265: 12.2762\n","f266: 13.9417\n","f267: 28.2846\n","f268: 16.3411\n","f269: 12.4784\n","f270: 62.5612\n","f271: 15.0961\n","f272: 10.9485\n","f273: 17.2267\n","f274: 10.9534\n","f275: 14.7677\n","f276: 45.9075\n","f277: 47.0570\n","f278: 1.0859\n","f282: 94.0985\n","f283: 78.2225\n","f284: 4.7583\n","f285: 2.7677\n","f286: 2.7467\n","f287: 5.0120\n","f288: 8.6913\n","f289: 27.1226\n","f290: 62.8303\n","f291: 13.5630\n","f292: 6.5094\n","f293: 9.0508\n","f294: 26.3945\n","f295: 15.9136\n","f296: 18.1141\n","f297: 9.0323\n","f298: 27.3485\n","f299: 17.7738\n","f300: 21.8334\n","f301: 23.6321\n","f302: 6.5585\n","f303: 6.6059\n","f304: 10.4802\n","f305: 6.6220\n","f306: 0.9552\n","f311: 4.7298\n","f312: 8.6353\n","f313: 2.2160\n","f314: 7.7032\n","f315: 19.8846\n","f316: 16.3442\n","f317: 21.3251\n","f318: 42.9395\n","f319: 36.7700\n","f320: 14.9993\n","f321: 16.3582\n","f322: 16.1664\n","f323: 11.5904\n","f324: 9.1749\n","f325: 8.4935\n","f326: 21.4358\n","f327: 20.6090\n","f328: 51.8839\n","f329: 38.5926\n","f330: 16.5608\n","f331: 16.5283\n","f332: 2.8322\n","f333: 5.8902\n","f334: 4.5583\n","f339: 6.8049\n","f340: 2.8053\n","f341: 4.0804\n","f342: 9.7204\n","f343: 57.4826\n","f344: 43.1068\n","f345: 22.7444\n","f346: 73.1122\n","f347: 28.4354\n","f348: 12.1237\n","f349: 29.2773\n","f350: 84.0824\n","f351: 24.0373\n","f352: 7.7523\n","f353: 12.9094\n","f354: 13.6164\n","f355: 10.0282\n","f356: 20.4187\n","f357: 12.5372\n","f358: 107.3103\n","f359: 15.1830\n","f360: 3.1243\n","f361: 2.4895\n","f362: 0.8541\n","f367: 3.6036\n","f368: 4.3469\n","f369: 6.3998\n","f370: 23.6202\n","f371: 21.2964\n","f372: 8.8781\n","f373: 16.9174\n","f374: 16.6553\n","f375: 27.4193\n","f376: 20.2048\n","f377: 17.7988\n","f378: 41.7666\n","f379: 14.3428\n","f380: 30.3720\n","f381: 12.4792\n","f382: 8.0094\n","f383: 7.2230\n","f384: 7.0446\n","f385: 4.0915\n","f386: 27.3201\n","f387: 8.0133\n","f388: 4.5510\n","f389: 3.9727\n","f395: 1.0564\n","f396: 2.7307\n","f397: 3.3588\n","f398: 7.7882\n","f399: 10.5109\n","f400: 36.9772\n","f401: 17.6783\n","f402: 15.5905\n","f403: 11.0703\n","f404: 62.5719\n","f405: 110.9349\n","f406: 71.7388\n","f407: 123.0399\n","f408: 10.4633\n","f409: 15.4874\n","f410: 94.0787\n","f411: 5.9839\n","f412: 3.1602\n","f413: 4.7883\n","f414: 5.1289\n","f415: 6.4570\n","f416: 5.2767\n","f417: 4.4799\n","f418: 1.4879\n","f423: 2.0719\n","f424: 1.6355\n","f425: 5.9597\n","f426: 6.3903\n","f427: 12.8335\n","f428: 18.7241\n","f429: 36.3825\n","f430: 15.6529\n","f431: 19.6650\n","f432: 36.5810\n","f433: 26.7623\n","f434: 20.6771\n","f435: 147.2637\n","f436: 9.0083\n","f437: 125.1990\n","f438: 46.6285\n","f439: 8.0594\n","f440: 5.1236\n","f441: 7.9425\n","f442: 13.1925\n","f443: 5.7459\n","f444: 4.2763\n","f445: 5.7849\n","f446: 1.8059\n","f451: 2.6026\n","f452: 1.4489\n","f453: 14.6649\n","f454: 4.7521\n","f455: 18.9040\n","f456: 8.7327\n","f457: 35.4027\n","f458: 15.1882\n","f459: 12.6050\n","f460: 8.7732\n","f461: 9.7066\n","f462: 19.9887\n","f463: 15.9501\n","f464: 14.1196\n","f465: 10.8575\n","f466: 17.2114\n","f467: 8.3483\n","f468: 5.9346\n","f469: 8.4656\n","f470: 12.2120\n","f471: 9.0441\n","f472: 5.3632\n","f473: 36.0197\n","f474: 1.7033\n","f479: 4.6733\n","f480: 3.9017\n","f481: 5.9535\n","f482: 4.6096\n","f483: 6.3002\n","f484: 18.0122\n","f485: 21.4965\n","f486: 33.3220\n","f487: 51.9548\n","f488: 19.7939\n","f489: 65.1810\n","f490: 50.2430\n","f491: 6.7721\n","f492: 4.5382\n","f493: 9.9436\n","f494: 5.7185\n","f495: 4.6573\n","f496: 4.9234\n","f497: 7.9983\n","f498: 8.1101\n","f499: 15.1367\n","f500: 8.6696\n","f501: 4.3647\n","f506: 1.9242\n","f507: 11.6340\n","f508: 6.2592\n","f509: 5.0282\n","f510: 22.6916\n","f511: 28.5296\n","f512: 26.7632\n","f513: 10.0689\n","f514: 88.8582\n","f515: 21.6935\n","f516: 17.3617\n","f517: 37.6855\n","f518: 8.8690\n","f519: 7.9083\n","f520: 5.2080\n","f521: 8.9157\n","f522: 36.8677\n","f523: 9.8282\n","f524: 6.7852\n","f525: 8.6063\n","f526: 14.7703\n","f527: 48.9225\n","f528: 22.0278\n","f529: 4.8828\n","f535: 5.7137\n","f536: 7.0407\n","f537: 13.1702\n","f538: 18.0334\n","f539: 18.4231\n","f540: 29.5470\n","f541: 39.6404\n","f542: 50.9440\n","f543: 48.7140\n","f544: 12.1671\n","f545: 9.4475\n","f546: 5.2195\n","f547: 4.8077\n","f548: 7.6499\n","f549: 8.8825\n","f550: 49.7577\n","f551: 11.3646\n","f552: 5.1716\n","f553: 15.7455\n","f554: 7.4476\n","f555: 21.9853\n","f556: 21.1628\n","f557: 3.2399\n","f563: 26.6627\n","f564: 34.9070\n","f565: 5.6239\n","f566: 9.4191\n","f567: 26.6158\n","f568: 23.4806\n","f569: 88.8403\n","f570: 23.6642\n","f571: 8.6754\n","f572: 15.6180\n","f573: 8.9212\n","f574: 6.0636\n","f575: 7.0243\n","f576: 4.9948\n","f577: 6.8833\n","f578: 29.1139\n","f579: 7.8608\n","f580: 4.6046\n","f581: 15.0796\n","f582: 83.2260\n","f583: 154.6680\n","f584: 2.8871\n","f585: 1.5923\n","f586: 3.0602\n","f591: 1.3455\n","f592: 4.4798\n","f593: 1.4073\n","f594: 9.0211\n","f595: 10.1792\n","f596: 39.9981\n","f597: 48.1849\n","f598: 32.2573\n","f599: 6.6069\n","f600: 6.7892\n","f601: 4.9094\n","f602: 7.1002\n","f603: 6.0919\n","f604: 5.2738\n","f605: 6.6541\n","f606: 6.7160\n","f607: 7.5744\n","f608: 9.9585\n","f609: 3.0018\n","f610: 8.0598\n","f611: 7.3808\n","f612: 1.4803\n","f613: 0.8718\n","f619: 1.2501\n","f620: 2.1621\n","f621: 2.9913\n","f622: 13.8460\n","f623: 17.1584\n","f624: 13.6854\n","f625: 11.0508\n","f626: 10.0293\n","f627: 9.7123\n","f628: 10.5156\n","f629: 7.2655\n","f630: 4.9641\n","f631: 4.7292\n","f632: 4.8173\n","f633: 6.0537\n","f634: 6.2701\n","f635: 5.6732\n","f636: 7.8701\n","f637: 6.5383\n","f638: 18.5901\n","f639: 2.5945\n","f640: 1.6168\n","f641: 0.8094\n","f647: 1.6072\n","f648: 5.5978\n","f649: 2.4132\n","f650: 3.2921\n","f651: 3.8600\n","f652: 18.9814\n","f653: 14.2272\n","f654: 11.4434\n","f655: 20.7923\n","f656: 65.2674\n","f657: 116.1350\n","f658: 11.8548\n","f659: 13.6104\n","f660: 17.8554\n","f661: 9.7560\n","f662: 6.5318\n","f663: 5.7254\n","f664: 2.3749\n","f665: 2.8169\n","f666: 2.6234\n","f667: 5.4133\n","f668: 3.1763\n","f675: 2.6317\n","f676: 3.4738\n","f677: 3.8820\n","f678: 3.2783\n","f679: 3.3181\n","f680: 3.5551\n","f681: 10.9617\n","f682: 5.8222\n","f683: 7.1301\n","f684: 13.2093\n","f685: 9.4067\n","f686: 7.4651\n","f687: 14.0103\n","f688: 11.0036\n","f689: 4.9498\n","f690: 4.7232\n","f691: 3.5791\n","f692: 2.1009\n","f693: 3.7661\n","f694: 3.7775\n","f695: 7.5162\n","f696: 2.3402\n","f704: 1.2306\n","f705: 15.9986\n","f706: 17.2972\n","f707: 12.7266\n","f708: 78.0309\n","f709: 54.4384\n","f710: 6.3877\n","f711: 115.3135\n","f712: 8.7907\n","f713: 106.6715\n","f714: 48.9590\n","f715: 16.1612\n","f716: 64.9272\n","f717: 47.5153\n","f718: 6.8131\n","f719: 65.8348\n","f720: 47.6570\n","f721: 4.1445\n","f722: 31.2068\n","f732: 0.8319\n","f733: 1.1214\n","f734: 0.7946\n","f735: 1.1603\n","f736: 2.7738\n","f737: 1.7816\n","f738: 15.6559\n","f739: 2.9578\n","f740: 177.8944\n","f741: 13.6482\n","f742: 108.5351\n","f743: 78.4583\n","f744: 12.7619\n","f745: 65.9772\n","f746: 4.1137\n","f747: 3.5373\n","f748: 3.0128\n","f749: 6.3943\n","f750: 0.9557\n","\n","Feature Importances (Booster by Weight):\n","f43: 1.0\n","f44: 4.0\n","f45: 3.0\n","f66: 5.0\n","f67: 2.0\n","f68: 12.0\n","f69: 28.0\n","f70: 7.0\n","f71: 9.0\n","f72: 8.0\n","f73: 10.0\n","f74: 3.0\n","f75: 2.0\n","f76: 1.0\n","f91: 5.0\n","f92: 18.0\n","f93: 37.0\n","f94: 21.0\n","f95: 24.0\n","f96: 45.0\n","f97: 56.0\n","f98: 58.0\n","f99: 44.0\n","f100: 42.0\n","f101: 48.0\n","f102: 40.0\n","f103: 60.0\n","f104: 41.0\n","f105: 14.0\n","f106: 13.0\n","f107: 16.0\n","f108: 2.0\n","f117: 5.0\n","f118: 1.0\n","f119: 12.0\n","f120: 20.0\n","f121: 48.0\n","f122: 33.0\n","f123: 63.0\n","f124: 77.0\n","f125: 115.0\n","f126: 80.0\n","f127: 104.0\n","f128: 119.0\n","f129: 84.0\n","f130: 73.0\n","f131: 75.0\n","f132: 61.0\n","f133: 52.0\n","f134: 38.0\n","f135: 28.0\n","f136: 17.0\n","f137: 7.0\n","f143: 1.0\n","f144: 3.0\n","f145: 21.0\n","f146: 37.0\n","f147: 30.0\n","f148: 54.0\n","f149: 56.0\n","f150: 54.0\n","f151: 65.0\n","f152: 97.0\n","f153: 129.0\n","f154: 135.0\n","f155: 147.0\n","f156: 120.0\n","f157: 90.0\n","f158: 106.0\n","f159: 73.0\n","f160: 49.0\n","f161: 46.0\n","f162: 51.0\n","f163: 48.0\n","f164: 26.0\n","f165: 15.0\n","f166: 1.0\n","f171: 1.0\n","f172: 2.0\n","f173: 21.0\n","f174: 26.0\n","f175: 44.0\n","f176: 56.0\n","f177: 56.0\n","f178: 63.0\n","f179: 130.0\n","f180: 128.0\n","f181: 114.0\n","f182: 151.0\n","f183: 162.0\n","f184: 160.0\n","f185: 163.0\n","f186: 114.0\n","f187: 76.0\n","f188: 73.0\n","f189: 70.0\n","f190: 50.0\n","f191: 70.0\n","f192: 37.0\n","f193: 12.0\n","f199: 2.0\n","f200: 10.0\n","f201: 58.0\n","f202: 35.0\n","f203: 51.0\n","f204: 66.0\n","f205: 93.0\n","f206: 106.0\n","f207: 111.0\n","f208: 146.0\n","f209: 142.0\n","f210: 168.0\n","f211: 146.0\n","f212: 180.0\n","f213: 194.0\n","f214: 172.0\n","f215: 116.0\n","f216: 77.0\n","f217: 72.0\n","f218: 67.0\n","f219: 61.0\n","f220: 50.0\n","f221: 11.0\n","f222: 4.0\n","f227: 3.0\n","f228: 23.0\n","f229: 53.0\n","f230: 47.0\n","f231: 41.0\n","f232: 52.0\n","f233: 83.0\n","f234: 97.0\n","f235: 120.0\n","f236: 138.0\n","f237: 117.0\n","f238: 174.0\n","f239: 146.0\n","f240: 190.0\n","f241: 174.0\n","f242: 126.0\n","f243: 134.0\n","f244: 92.0\n","f245: 92.0\n","f246: 47.0\n","f247: 68.0\n","f248: 71.0\n","f249: 15.0\n","f250: 1.0\n","f255: 8.0\n","f256: 35.0\n","f257: 36.0\n","f258: 32.0\n","f259: 52.0\n","f260: 77.0\n","f261: 109.0\n","f262: 107.0\n","f263: 146.0\n","f264: 140.0\n","f265: 154.0\n","f266: 216.0\n","f267: 228.0\n","f268: 222.0\n","f269: 181.0\n","f270: 146.0\n","f271: 186.0\n","f272: 147.0\n","f273: 78.0\n","f274: 70.0\n","f275: 84.0\n","f276: 82.0\n","f277: 18.0\n","f278: 1.0\n","f282: 1.0\n","f283: 6.0\n","f284: 7.0\n","f285: 26.0\n","f286: 62.0\n","f287: 59.0\n","f288: 87.0\n","f289: 120.0\n","f290: 166.0\n","f291: 140.0\n","f292: 192.0\n","f293: 223.0\n","f294: 263.0\n","f295: 263.0\n","f296: 243.0\n","f297: 185.0\n","f298: 160.0\n","f299: 142.0\n","f300: 150.0\n","f301: 89.0\n","f302: 90.0\n","f303: 65.0\n","f304: 72.0\n","f305: 20.0\n","f306: 2.0\n","f311: 1.0\n","f312: 33.0\n","f313: 27.0\n","f314: 57.0\n","f315: 106.0\n","f316: 142.0\n","f317: 163.0\n","f318: 170.0\n","f319: 181.0\n","f320: 196.0\n","f321: 263.0\n","f322: 281.0\n","f323: 214.0\n","f324: 201.0\n","f325: 201.0\n","f326: 174.0\n","f327: 146.0\n","f328: 134.0\n","f329: 97.0\n","f330: 121.0\n","f331: 78.0\n","f332: 52.0\n","f333: 20.0\n","f334: 1.0\n","f339: 6.0\n","f340: 19.0\n","f341: 60.0\n","f342: 73.0\n","f343: 91.0\n","f344: 136.0\n","f345: 207.0\n","f346: 213.0\n","f347: 194.0\n","f348: 255.0\n","f349: 222.0\n","f350: 276.0\n","f351: 230.0\n","f352: 195.0\n","f353: 197.0\n","f354: 167.0\n","f355: 174.0\n","f356: 124.0\n","f357: 136.0\n","f358: 136.0\n","f359: 87.0\n","f360: 35.0\n","f361: 10.0\n","f362: 2.0\n","f367: 4.0\n","f368: 27.0\n","f369: 50.0\n","f370: 87.0\n","f371: 127.0\n","f372: 143.0\n","f373: 152.0\n","f374: 223.0\n","f375: 209.0\n","f376: 209.0\n","f377: 232.0\n","f378: 234.0\n","f379: 197.0\n","f380: 182.0\n","f381: 165.0\n","f382: 124.0\n","f383: 131.0\n","f384: 94.0\n","f385: 79.0\n","f386: 104.0\n","f387: 93.0\n","f388: 41.0\n","f389: 9.0\n","f395: 4.0\n","f396: 21.0\n","f397: 43.0\n","f398: 89.0\n","f399: 155.0\n","f400: 198.0\n","f401: 193.0\n","f402: 198.0\n","f403: 181.0\n","f404: 154.0\n","f405: 202.0\n","f406: 205.0\n","f407: 181.0\n","f408: 183.0\n","f409: 202.0\n","f410: 151.0\n","f411: 132.0\n","f412: 95.0\n","f413: 70.0\n","f414: 63.0\n","f415: 65.0\n","f416: 38.0\n","f417: 22.0\n","f418: 3.0\n","f423: 2.0\n","f424: 19.0\n","f425: 63.0\n","f426: 92.0\n","f427: 164.0\n","f428: 174.0\n","f429: 235.0\n","f430: 190.0\n","f431: 178.0\n","f432: 176.0\n","f433: 188.0\n","f434: 268.0\n","f435: 168.0\n","f436: 151.0\n","f437: 149.0\n","f438: 174.0\n","f439: 154.0\n","f440: 108.0\n","f441: 68.0\n","f442: 75.0\n","f443: 37.0\n","f444: 49.0\n","f445: 13.0\n","f446: 2.0\n","f451: 6.0\n","f452: 16.0\n","f453: 42.0\n","f454: 87.0\n","f455: 148.0\n","f456: 183.0\n","f457: 215.0\n","f458: 177.0\n","f459: 199.0\n","f460: 219.0\n","f461: 207.0\n","f462: 223.0\n","f463: 138.0\n","f464: 152.0\n","f465: 116.0\n","f466: 122.0\n","f467: 126.0\n","f468: 83.0\n","f469: 60.0\n","f470: 71.0\n","f471: 46.0\n","f472: 26.0\n","f473: 25.0\n","f474: 2.0\n","f479: 6.0\n","f480: 15.0\n","f481: 43.0\n","f482: 52.0\n","f483: 112.0\n","f484: 142.0\n","f485: 192.0\n","f486: 201.0\n","f487: 219.0\n","f488: 214.0\n","f489: 237.0\n","f490: 186.0\n","f491: 159.0\n","f492: 137.0\n","f493: 115.0\n","f494: 98.0\n","f495: 78.0\n","f496: 57.0\n","f497: 58.0\n","f498: 30.0\n","f499: 35.0\n","f500: 24.0\n","f501: 22.0\n","f506: 1.0\n","f507: 14.0\n","f508: 17.0\n","f509: 42.0\n","f510: 59.0\n","f511: 125.0\n","f512: 172.0\n","f513: 139.0\n","f514: 189.0\n","f515: 200.0\n","f516: 265.0\n","f517: 211.0\n","f518: 165.0\n","f519: 123.0\n","f520: 131.0\n","f521: 128.0\n","f522: 77.0\n","f523: 76.0\n","f524: 44.0\n","f525: 56.0\n","f526: 27.0\n","f527: 23.0\n","f528: 47.0\n","f529: 16.0\n","f535: 8.0\n","f536: 29.0\n","f537: 33.0\n","f538: 54.0\n","f539: 94.0\n","f540: 131.0\n","f541: 142.0\n","f542: 194.0\n","f543: 180.0\n","f544: 198.0\n","f545: 134.0\n","f546: 123.0\n","f547: 119.0\n","f548: 108.0\n","f549: 80.0\n","f550: 104.0\n","f551: 88.0\n","f552: 68.0\n","f553: 61.0\n","f554: 47.0\n","f555: 36.0\n","f556: 32.0\n","f557: 7.0\n","f563: 17.0\n","f564: 26.0\n","f565: 29.0\n","f566: 49.0\n","f567: 88.0\n","f568: 114.0\n","f569: 154.0\n","f570: 143.0\n","f571: 152.0\n","f572: 149.0\n","f573: 142.0\n","f574: 127.0\n","f575: 135.0\n","f576: 82.0\n","f577: 92.0\n","f578: 74.0\n","f579: 73.0\n","f580: 57.0\n","f581: 69.0\n","f582: 59.0\n","f583: 21.0\n","f584: 13.0\n","f585: 2.0\n","f586: 1.0\n","f591: 6.0\n","f592: 25.0\n","f593: 22.0\n","f594: 70.0\n","f595: 78.0\n","f596: 89.0\n","f597: 122.0\n","f598: 119.0\n","f599: 108.0\n","f600: 109.0\n","f601: 109.0\n","f602: 98.0\n","f603: 91.0\n","f604: 74.0\n","f605: 92.0\n","f606: 90.0\n","f607: 78.0\n","f608: 57.0\n","f609: 45.0\n","f610: 39.0\n","f611: 33.0\n","f612: 13.0\n","f613: 1.0\n","f619: 3.0\n","f620: 50.0\n","f621: 32.0\n","f622: 58.0\n","f623: 58.0\n","f624: 68.0\n","f625: 104.0\n","f626: 111.0\n","f627: 113.0\n","f628: 103.0\n","f629: 123.0\n","f630: 116.0\n","f631: 97.0\n","f632: 97.0\n","f633: 69.0\n","f634: 65.0\n","f635: 85.0\n","f636: 50.0\n","f637: 43.0\n","f638: 28.0\n","f639: 8.0\n","f640: 9.0\n","f641: 1.0\n","f647: 4.0\n","f648: 25.0\n","f649: 43.0\n","f650: 53.0\n","f651: 57.0\n","f652: 53.0\n","f653: 97.0\n","f654: 107.0\n","f655: 93.0\n","f656: 141.0\n","f657: 113.0\n","f658: 123.0\n","f659: 123.0\n","f660: 88.0\n","f661: 83.0\n","f662: 107.0\n","f663: 56.0\n","f664: 49.0\n","f665: 37.0\n","f666: 10.0\n","f667: 19.0\n","f668: 9.0\n","f675: 1.0\n","f676: 16.0\n","f677: 54.0\n","f678: 69.0\n","f679: 67.0\n","f680: 59.0\n","f681: 90.0\n","f682: 122.0\n","f683: 85.0\n","f684: 78.0\n","f685: 107.0\n","f686: 97.0\n","f687: 75.0\n","f688: 65.0\n","f689: 75.0\n","f690: 49.0\n","f691: 61.0\n","f692: 35.0\n","f693: 19.0\n","f694: 20.0\n","f695: 5.0\n","f696: 3.0\n","f704: 2.0\n","f705: 13.0\n","f706: 26.0\n","f707: 45.0\n","f708: 35.0\n","f709: 64.0\n","f710: 51.0\n","f711: 58.0\n","f712: 64.0\n","f713: 43.0\n","f714: 47.0\n","f715: 45.0\n","f716: 32.0\n","f717: 42.0\n","f718: 31.0\n","f719: 26.0\n","f720: 14.0\n","f721: 7.0\n","f722: 7.0\n","f732: 1.0\n","f733: 8.0\n","f734: 7.0\n","f735: 1.0\n","f736: 3.0\n","f737: 4.0\n","f738: 16.0\n","f739: 9.0\n","f740: 26.0\n","f741: 15.0\n","f742: 21.0\n","f743: 34.0\n","f744: 7.0\n","f745: 19.0\n","f746: 5.0\n","f747: 3.0\n","f748: 2.0\n","f749: 3.0\n","f750: 1.0\n"]}]},{"cell_type":"markdown","source":["\n","# LightGBM\n","\n","LightGBM is a high-performance gradient boosting framework optimized for **speed and memory efficiency**, particularly on **large datasets**. It introduces three major improvements over traditional boosting methods:\n","\n","1. **Leaf-Wise Tree Growth**: Prioritizes splits that yield the highest loss reduction.\n","2. **Gradient-Based One-Side Sampling (GOSS)**: Efficiently reduces training data size while preserving crucial gradient information.\n","3. **Exclusive Feature Bundling (EFB)**: Compresses sparse high-dimensional data by bundling mutually exclusive features.\n","\n","\n","\n","## Leaf-Wise Tree Growth\n","\n","Instead of growing trees **depth-wise** (splitting all leaves at a given depth before moving to the next), LightGBM grows trees **leaf-wise (best-first)**:\n","\n","- At each step, it **splits the leaf** that results in the highest loss reduction.\n","- This allows LightGBM to **focus on the most informative regions** of the feature space, leading to faster error minimization.\n","- **Potential drawback**: This can lead to **very deep trees** in localized regions, increasing the risk of overfitting. Regularization via `max_depth` and `num_leaves` is important.\n","\n","\n","\n","##  Gradient-Based One-Side Sampling (GOSS)\n","\n","###  High Training Cost on Large Data\n","\n","Gradient boosting requires computing **gradients and Hessians for all data points** in every iteration to determine the best tree split. When datasets are **very large**, this becomes computationally expensive. A naive solution is **random subsampling**, but this risks discarding important information.\n","\n","**GOSS** selectively **keeps** the most **informative** data points:\n","\n","1. **Large-gradient examples** (hard-to-predict points) are **fully retained** because they contribute most to model updates.\n","2. **Small-gradient examples** (already well-predicted points) are **partially discarded** via random subsampling to save computation.\n","3. **Rescaling** ensures the overall gradient statistics remain correct.\n","\n","By keeping the strongest gradient signals while reducing redundancy, LightGBM **reduces computational cost while maintaining accuracy**.\n","\n","\n","\n","### The GOSS Algorithm\n","\n","1. **Compute Gradients**:  \n","   Compute the first-order gradient $g_i$ for each training instance $i$.\n","\n","2. **Sort by Gradient Magnitude**:  \n","   Rank all data points by $|g_i|$ in descending order.\n","\n","3. **Select Two Sets**:  \n","   - $A$: The top $a$ fraction of data points with the **largest** gradients (fully retained).\n","   - $B'$: The bottom $(1-a)$ fraction of small-gradient points (potentially sampled).\n","\n","4. **Sample from the Small-Gradient Set**:  \n","   From $B'$, randomly select a fraction $b$. The final training set becomes:\n","   $$\n","   S = A \\cup B\n","   $$\n","   - $|A| = aN$ (all kept).\n","   - $|B| = b(1-a)N$ (randomly sampled).\n","\n","5. **Re-Scale Small Gradients**:  \n","   To correct for sampling bias, each gradient in $B$ is **upweighted**:\n","   $$\n","   g'_i = \\alpha g_i, \\quad \\text{where } \\alpha = \\frac{1-a}{b}.\n","   $$\n","   This ensures the total gradient sum remains unbiased.\n","\n","6. **Train the Tree on the Reduced Set**:  \n","   LightGBM constructs the tree using only $S$, significantly reducing computation.\n","\n","\n","## Exclusive Feature Bundling (EFB)\n","\n","\n","Many real-world datasets have **thousands or millions of features**, particularly in:\n","\n","- **One-hot encodings** of categorical variables.\n","- **Sparse matrices** (e.g., text data, recommender systems).\n","\n","Processing these features **individually** is expensive in terms of **memory** and **computation**.\n","\n","\n","**EFB** identifies **mutually exclusive** features—features that are **never nonzero at the same time**—and **combines them into a single feature**.\n","\n","For example:\n","\n","| Feature 1 | Feature 2 | Feature 3 |\n","|-----------|-----------|-----------|\n","| 1         | 0         | 0         |\n","| 0         | 1         | 0         |\n","| 0         | 0         | 1         |\n","\n","Since at most **one feature is nonzero per row**, these can be **merged into a single feature index**, reducing memory usage and computation.\n","\n","\n","\n","###  How EFB Works\n","\n","1. **Identify Groups of Exclusive Features**:\n","   - Construct a **graph** where each feature is a node.\n","   - Connect features if they **overlap too much** (i.e., are nonzero in the same rows).\n","   - Solve this as a **graph coloring** problem to minimize the number of feature groups.\n","\n","2. **Bundle Features into a Shared Histogram**:\n","   - Each group of features is **combined into a single histogram**.\n","   - Within the histogram, each feature **uses a separate bin range**.\n","   - This drastically **reduces the number of histograms** needed.\n"],"metadata":{"id":"D96eAi7Cx_TF"}},{"cell_type":"code","source":["import lightgbm as lgb\n","\n","clf = lgb.LGBMClassifier(\n","    objective='multiclass',\n","    num_class=10,\n","    learning_rate=0.1,\n","    n_estimators=100,\n","    subsample=0.8,\n","    colsample_bytree=0.8,\n","    random_state=42\n",")\n","\n","# Train the classifier with early stopping.\n","# The eval_set allows the model to monitor performance on the test set and stop early if no improvement is seen.\n","clf.fit(\n","    X_train, y_train,\n","    eval_set=[(X_test, y_test)],\n","    eval_metric='multi_logloss', callbacks=[lgb.log_evaluation(period=1)]\n",")\n","\n","# Make predictions on the test set and evaluate accuracy.\n","y_pred = clf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"LGBMClassifier Accuracy on MNIST: {:.2f}%\".format(accuracy * 100))\n","\n"],"metadata":{"id":"bMxdDqrKyBJ0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1758898795801,"user_tz":240,"elapsed":151997,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"8ebbe296-d375-4b30-c576-7540ae8d2d3a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.730158 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 109563\n","[LightGBM] [Info] Number of data points in the train set: 60000, number of used features: 632\n","[LightGBM] [Info] Start training from score -2.316515\n","[LightGBM] [Info] Start training from score -2.184506\n","[LightGBM] [Info] Start training from score -2.304086\n","[LightGBM] [Info] Start training from score -2.282619\n","[LightGBM] [Info] Start training from score -2.328074\n","[LightGBM] [Info] Start training from score -2.405911\n","[LightGBM] [Info] Start training from score -2.320410\n","[LightGBM] [Info] Start training from score -2.261603\n","[LightGBM] [Info] Start training from score -2.327903\n","[LightGBM] [Info] Start training from score -2.308603\n","[1]\tvalid_0's multi_logloss: 1.69342\n","[2]\tvalid_0's multi_logloss: 1.39071\n","[3]\tvalid_0's multi_logloss: 1.17623\n","[4]\tvalid_0's multi_logloss: 1.01611\n","[5]\tvalid_0's multi_logloss: 0.889818\n","[6]\tvalid_0's multi_logloss: 0.788155\n","[7]\tvalid_0's multi_logloss: 0.702904\n","[8]\tvalid_0's multi_logloss: 0.630863\n","[9]\tvalid_0's multi_logloss: 0.57012\n","[10]\tvalid_0's multi_logloss: 0.519245\n","[11]\tvalid_0's multi_logloss: 0.475545\n","[12]\tvalid_0's multi_logloss: 0.438017\n","[13]\tvalid_0's multi_logloss: 0.405358\n","[14]\tvalid_0's multi_logloss: 0.377099\n","[15]\tvalid_0's multi_logloss: 0.350643\n","[16]\tvalid_0's multi_logloss: 0.327642\n","[17]\tvalid_0's multi_logloss: 0.307784\n","[18]\tvalid_0's multi_logloss: 0.289424\n","[19]\tvalid_0's multi_logloss: 0.273856\n","[20]\tvalid_0's multi_logloss: 0.259176\n","[21]\tvalid_0's multi_logloss: 0.246381\n","[22]\tvalid_0's multi_logloss: 0.234679\n","[23]\tvalid_0's multi_logloss: 0.224314\n","[24]\tvalid_0's multi_logloss: 0.214832\n","[25]\tvalid_0's multi_logloss: 0.206223\n","[26]\tvalid_0's multi_logloss: 0.197706\n","[27]\tvalid_0's multi_logloss: 0.190741\n","[28]\tvalid_0's multi_logloss: 0.184099\n","[29]\tvalid_0's multi_logloss: 0.178566\n","[30]\tvalid_0's multi_logloss: 0.172978\n","[31]\tvalid_0's multi_logloss: 0.167398\n","[32]\tvalid_0's multi_logloss: 0.16269\n","[33]\tvalid_0's multi_logloss: 0.158274\n","[34]\tvalid_0's multi_logloss: 0.153658\n","[35]\tvalid_0's multi_logloss: 0.149617\n","[36]\tvalid_0's multi_logloss: 0.145933\n","[37]\tvalid_0's multi_logloss: 0.142442\n","[38]\tvalid_0's multi_logloss: 0.139161\n","[39]\tvalid_0's multi_logloss: 0.136257\n","[40]\tvalid_0's multi_logloss: 0.13311\n","[41]\tvalid_0's multi_logloss: 0.130613\n","[42]\tvalid_0's multi_logloss: 0.128227\n","[43]\tvalid_0's multi_logloss: 0.125847\n","[44]\tvalid_0's multi_logloss: 0.123617\n","[45]\tvalid_0's multi_logloss: 0.121708\n","[46]\tvalid_0's multi_logloss: 0.119705\n","[47]\tvalid_0's multi_logloss: 0.117749\n","[48]\tvalid_0's multi_logloss: 0.115856\n","[49]\tvalid_0's multi_logloss: 0.11432\n","[50]\tvalid_0's multi_logloss: 0.112538\n","[51]\tvalid_0's multi_logloss: 0.110723\n","[52]\tvalid_0's multi_logloss: 0.109181\n","[53]\tvalid_0's multi_logloss: 0.107782\n","[54]\tvalid_0's multi_logloss: 0.10617\n","[55]\tvalid_0's multi_logloss: 0.104572\n","[56]\tvalid_0's multi_logloss: 0.103284\n","[57]\tvalid_0's multi_logloss: 0.102233\n","[58]\tvalid_0's multi_logloss: 0.100874\n","[59]\tvalid_0's multi_logloss: 0.099664\n","[60]\tvalid_0's multi_logloss: 0.0984843\n","[61]\tvalid_0's multi_logloss: 0.0975601\n","[62]\tvalid_0's multi_logloss: 0.0965734\n","[63]\tvalid_0's multi_logloss: 0.0954575\n","[64]\tvalid_0's multi_logloss: 0.0947181\n","[65]\tvalid_0's multi_logloss: 0.093664\n","[66]\tvalid_0's multi_logloss: 0.092846\n","[67]\tvalid_0's multi_logloss: 0.0921629\n","[68]\tvalid_0's multi_logloss: 0.0911584\n","[69]\tvalid_0's multi_logloss: 0.0903426\n","[70]\tvalid_0's multi_logloss: 0.0897133\n","[71]\tvalid_0's multi_logloss: 0.0891572\n","[72]\tvalid_0's multi_logloss: 0.0882945\n","[73]\tvalid_0's multi_logloss: 0.0877773\n","[74]\tvalid_0's multi_logloss: 0.0871566\n","[75]\tvalid_0's multi_logloss: 0.086669\n","[76]\tvalid_0's multi_logloss: 0.0861147\n","[77]\tvalid_0's multi_logloss: 0.0855842\n","[78]\tvalid_0's multi_logloss: 0.0851468\n","[79]\tvalid_0's multi_logloss: 0.0846742\n","[80]\tvalid_0's multi_logloss: 0.0840745\n","[81]\tvalid_0's multi_logloss: 0.0837333\n","[82]\tvalid_0's multi_logloss: 0.0832895\n","[83]\tvalid_0's multi_logloss: 0.0827521\n","[84]\tvalid_0's multi_logloss: 0.0823838\n","[85]\tvalid_0's multi_logloss: 0.0817448\n","[86]\tvalid_0's multi_logloss: 0.0813676\n","[87]\tvalid_0's multi_logloss: 0.0808083\n","[88]\tvalid_0's multi_logloss: 0.0804309\n","[89]\tvalid_0's multi_logloss: 0.0800129\n","[90]\tvalid_0's multi_logloss: 0.0796566\n","[91]\tvalid_0's multi_logloss: 0.0794271\n","[92]\tvalid_0's multi_logloss: 0.0790747\n","[93]\tvalid_0's multi_logloss: 0.0787878\n","[94]\tvalid_0's multi_logloss: 0.0785193\n","[95]\tvalid_0's multi_logloss: 0.0782567\n","[96]\tvalid_0's multi_logloss: 0.0780346\n","[97]\tvalid_0's multi_logloss: 0.0778505\n","[98]\tvalid_0's multi_logloss: 0.0774792\n","[99]\tvalid_0's multi_logloss: 0.0770987\n","[100]\tvalid_0's multi_logloss: 0.0769959\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMClassifier was fitted with feature names\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["LGBMClassifier Accuracy on MNIST: 97.59%\n"]}]},{"cell_type":"markdown","source":["# HistGradientBoosting\n","\n","The **HistGradientBoostingRegressor** is a histogram-based gradient boosting method that improves efficiency by binning continuous features and using histograms to optimize split selection. Its **three core innovations** are:\n","\n","1. **Histogram-Based Binning**: Converts continuous feature values into discrete bins, reducing computation time.\n","2. **Efficient Split Finding**: Uses precomputed histograms to quickly determine optimal splits.\n","3. **Regularization via Binning**: The discretization process reduces noise and improves generalization.\n","\n","\n","## Binning the Features\n","\n","Each feature $j$ is **mapped to a bin index** before training:\n","$$\n","b_{ij} = \\mathcal{B}_j(x_{ij}) \\in \\{0, 1, \\dots, B-1\\}.\n","$$\n","where:\n","- $ x_{ij} $ is the raw feature value.\n","- $ \\mathcal{B}_j $ is a **binning function** (e.g., uniform binning, quantile-based binning).\n","- $B$ is the **number of bins per feature** (typically much smaller than the number of unique values).\n","\n","### Advantages of Binning\n","\n","- **Computational Speed-Up**: The number of candidate splits is reduced from **unique values** to **only $B$ bins**.\n","- **Memory Efficiency**: Instead of storing raw feature values, we store **integer bin indices**, reducing memory usage.\n","- **Regularization Effect**: Binning smooths out **noise** in feature values, reducing overfitting.\n","\n"],"metadata":{"id":"1SoBlPtrem52"}},{"cell_type":"markdown","source":["# CatBoost\n","\n","CatBoost is a gradient boosting framework optimized for **handling categorical data** efficiently and **reducing overfitting**. It introduces three main innovations:\n","\n","1. **Oblivious (Symmetric) Decision Trees**: A highly regularized tree structure that ensures efficient computation.\n","2. **Ordered Target Statistics**: A principled method for encoding categorical features while preventing data leakage.\n","3. **Permutation-Driven Boosting**: A robust training procedure that stabilizes gradient estimates.\n","\n","\n","\n","##  Oblivious (Symmetric) Decision Trees\n","\n","\n","A tree $T(x)$ of depth $d$ is defined by:\n","- A set of features $\\{ f_1, f_2, \\dots, f_d \\}$ used for splitting.\n","- A set of thresholds $\\{ t_1, t_2, \\dots, t_d \\}$.\n","- Leaf values $\\{ l_0, l_1, \\dots, l_{2^d - 1} \\}$.\n","\n","For an input $x$, the decision at level $j$ is:\n","$$\n","b_j(x) = \\mathbb{I} \\{ x_{f_j} \\leq t_j \\}.\n","$$\n","The leaf index is computed as:\n","$$\n","\\ell(x) = \\sum_{j=1}^{d} 2^{j-1} \\, b_j(x).\n","$$\n","The final tree prediction is:\n","$$\n","T(x) = l_{\\ell(x)}.\n","$$\n","This **regularizes the model**, reducing variance and **speeding up inference**, as tree traversal is **bitwise-computable**.\n","\n","\n","\n","##  Ordered Target Statistics for Categorical Features\n","\n","###  Handling Categorical Features\n","\n","Most boosting frameworks convert categorical variables using **one-hot encoding** or **mean target encoding**. However:\n","- **One-hot encoding** creates **high-dimensional sparse features**.\n","- **Mean target encoding** suffers from **target leakage** if computed over the full dataset.\n","\n","CatBoost **avoids leakage** by computing categorical transformations **in an online manner** using **ordered statistics**.\n","\n","\n","\n","### Ordered Target Encoding\n","\n","For a categorical feature $C$, each category $c_i$ is transformed into:\n","$$\n","\\phi(c_i) = \\frac{\\sum_{j < i} \\mathbb{I}\\{ c_j = c_i \\} y_j + a \\gamma}{\\sum_{j < i} \\mathbb{I}\\{ c_j = c_i \\} + a}.\n","$$\n","where:\n","- **Summation is only over past samples** ($j < i$), ensuring **no information leakage**.\n","- **$a$** is a regularization parameter (prior weight).\n","- **$\\gamma$** is a prior value (e.g., global mean of $y$).\n","\n","This ensures that each sample **does not** see its own label during encoding.\n","\n","\n","\n","### Permutation-Driven Encoding\n","\n","Instead of **one fixed ordering**, CatBoost applies **multiple random permutations** of the dataset, computing:\n","$$\n","\\phi_{\\text{final}}(c_i) = \\frac{1}{P} \\sum_{p=1}^{P} \\phi^{(p)}(c_i),\n","$$\n","where $\\phi^{(p)}(c_i)$ is computed using permutation $p$.\n","\n","**Benefits:**\n","- **Reduces variance** in categorical transformations.\n","- **Improves generalization** across different training orderings.\n","\n","\n"],"metadata":{"id":"ZSiWqXoCdHlT"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, classification_report\n","\n","#########################################\n","# 1. Load and Preprocess the Adult Data #\n","#########################################\n","\n","# Fetch the Adult dataset from OpenML as a DataFrame.\n","adult = fetch_openml(\"adult\", version=2, as_frame=True)\n","df = adult.frame\n","\n","# Replace missing values represented by \"?\" with NaN, then drop rows with missing values.\n","df.replace(\"?\", np.nan, inplace=True)\n","df.dropna(inplace=True)\n","\n","# The target column is named \"class\" (income level)\n","target = \"class\"\n","\n","# Separate features and target.\n","X = df.drop(columns=[target])\n","y = df[target]\n","# Identify categorical features: here, we assume that columns with object dtype are categorical.\n","cat_features = X.select_dtypes(include=['category']).columns.tolist()\n","\n","# Perform a stratified train-test split to preserve class proportions.\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# For CatBoost, determine the indices of the categorical features (based on the training DataFrame).\n","cat_feature_indices = [X_train.columns.get_loc(col) for col in cat_features]\n","\n"],"metadata":{"id":"U2HZmCOnfnL9","executionInfo":{"status":"ok","timestamp":1758898944836,"user_tz":240,"elapsed":2990,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["cat_features"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DlgrF-lbEggv","executionInfo":{"status":"ok","timestamp":1758898965469,"user_tz":240,"elapsed":31,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"11c7a690-e949-4cd4-cc04-b84cc71eab86"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['workclass',\n"," 'education',\n"," 'marital-status',\n"," 'occupation',\n"," 'relationship',\n"," 'race',\n"," 'sex',\n"," 'native-country']"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["!pip install catboost"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XsZLXR8fEmP3","executionInfo":{"status":"ok","timestamp":1758898980159,"user_tz":240,"elapsed":13556,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"f55c704d-f80b-4ade-8945-f0c7a974b5c0"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting catboost\n","  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n","Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n","Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n","Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.4)\n","Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n","Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: catboost\n","Successfully installed catboost-1.2.8\n"]}]},{"cell_type":"code","source":["#########################################\n","# 2. Train and Evaluate CatBoost Model  #\n","#########################################\n","\n","from catboost import CatBoostClassifier\n","\n","# Initialize CatBoostClassifier.\n","# - iterations: Number of boosting rounds.\n","# - learning_rate: Shrinks the contribution of each tree.\n","# - depth: Maximum tree depth.\n","# - verbose: Prints progress messages.\n","cat_model = CatBoostClassifier(\n","    iterations=500,\n","    learning_rate=0.1,\n","    depth=6,\n","    random_seed=42,\n","    verbose=50\n",")\n","\n","# Train the model, passing the categorical feature indices.\n","cat_model.fit(X_train, y_train, cat_features=cat_feature_indices)\n","\n","# Predict on the test set.\n","y_pred_cat = cat_model.predict(X_test)\n","\n","# Evaluate CatBoost's performance.\n","acc_cat = accuracy_score(y_test, y_pred_cat)\n","print(\"CatBoost Accuracy:\", acc_cat)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k7Ks1jI4EdBe","executionInfo":{"status":"ok","timestamp":1758899047280,"user_tz":240,"elapsed":67118,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"4e776002-adef-4e98-bad3-75f89ab02c83"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["0:\tlearn: 0.6020203\ttotal: 198ms\tremaining: 1m 38s\n","50:\tlearn: 0.2928853\ttotal: 9.54s\tremaining: 1m 24s\n","100:\tlearn: 0.2817301\ttotal: 19.5s\tremaining: 1m 16s\n","150:\tlearn: 0.2741182\ttotal: 30.3s\tremaining: 1m 10s\n","200:\tlearn: 0.2691135\ttotal: 39.6s\tremaining: 59s\n","250:\tlearn: 0.2646418\ttotal: 47.6s\tremaining: 47.3s\n","300:\tlearn: 0.2607285\ttotal: 52.3s\tremaining: 34.6s\n","350:\tlearn: 0.2574644\ttotal: 55.4s\tremaining: 23.5s\n","400:\tlearn: 0.2535783\ttotal: 58.7s\tremaining: 14.5s\n","450:\tlearn: 0.2509368\ttotal: 1m 3s\tremaining: 6.88s\n","499:\tlearn: 0.2480219\ttotal: 1m 6s\tremaining: 0us\n","CatBoost Accuracy: 0.867551133222775\n"]}]},{"cell_type":"code","source":["\n","#########################################\n","# 3. Train and Evaluate LightGBM Model  #\n","#########################################\n","\n","import lightgbm as lgb\n","from lightgbm import LGBMClassifier\n","\n","# Initialize the LightGBM classifier.\n","lgb_model = LGBMClassifier(\n","    n_estimators=500,\n","    learning_rate=0.1,\n","    max_depth=6,\n","    random_state=42\n",")\n","\n","# Train the model.\n","# For LightGBM, pass the categorical feature names.\n","lgb_model.fit(X_train, y_train, categorical_feature=cat_features,callbacks=[lgb.log_evaluation(period=1)])\n","\n","# Predict on the test set.\n","y_pred_lgb = lgb_model.predict(X_test)\n","\n","# Evaluate LightGBM's performance.\n","acc_lgb = accuracy_score(y_test, y_pred_lgb)\n","print(\"LightGBM Accuracy:\", acc_lgb)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZOG1MYX6GUGa","executionInfo":{"status":"ok","timestamp":1758899056824,"user_tz":240,"elapsed":6828,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"8413648d-a75f-43e5-b03b-1c73b226ebf0"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["[LightGBM] [Info] Number of positive: 8966, number of negative: 27211\n","[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.007774 seconds.\n","You can set `force_row_wise=true` to remove the overhead.\n","And if memory is not enough, you can set `force_col_wise=true`.\n","[LightGBM] [Info] Total Bins 708\n","[LightGBM] [Info] Number of data points in the train set: 36177, number of used features: 14\n","[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.247837 -> initscore=-1.110182\n","[LightGBM] [Info] Start training from score -1.110182\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n","LightGBM Accuracy: 0.8652294085129906\n"]}]},{"cell_type":"code","source":["# lgb_model is a fitted LGBMClassifier/Regressor\n","booster = lgb_model.booster_            # LightGBM Booster\n","\n","# Importances (arrays aligned with feature names)\n","gain  = booster.feature_importance(importance_type='gain')\n","feat_names = booster.feature_name()\n","\n","# Nicely formatted table\n","import pandas as pd\n","imp = pd.DataFrame({\n","    'feature': feat_names,\n","    'gain': gain,\n","}).sort_values('gain', ascending=False)\n","\n","print(imp.to_string(index=False))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"24KWu3TUj4H-","executionInfo":{"status":"ok","timestamp":1758899277730,"user_tz":240,"elapsed":81,"user":{"displayName":"Ahmad Aghapour","userId":"10024107043211715538"}},"outputId":"b31e8bed-18e5-465e-eb49-2ccf1d543c96"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["       feature         gain\n","  relationship 33471.681778\n","  capital-gain 22692.301018\n"," education-num 16749.617897\n","           age 13056.510353\n","marital-status 10872.191314\n","    occupation 10154.502033\n","        fnlwgt  9744.494319\n","  capital-loss  7862.270918\n","hours-per-week  7184.608974\n","     workclass  1427.895170\n","     education  1229.338049\n","native-country  1134.529562\n","           sex   978.564814\n","          race   173.511303\n"]}]}]}